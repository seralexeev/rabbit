ARG BASE_IMAGE=nvcr.io/nvidia/l4t-jetpack:r36.3.0
FROM ${BASE_IMAGE}

# Argument for skipping pytorch installation.
ARG SKIP_PYTORCH_INSTALL=0
ENV SKIP_PYTORCH_INSTALL=${SKIP_PYTORCH_INSTALL}

# TZData goes first.
RUN apt-get update
ENV TZ=Europe/Berlin
ENV DEBIAN_FRONTEND=noninteractive
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
RUN apt-get update && apt-get install -y tzdata

# Install apt dependencies.
RUN apt update && apt-get --no-install-recommends install -y \
    # Source development \
    git jq build-essential \
    # System packages \
    sudo ssh gnupg wget curl unzip \
    # Package management tools \
    apt-utils software-properties-common \
    # Python bootstrap packages \
    python3-pip python3-venv \
    # Open3D system dependencies \
    # NOTE(alexmillane): Taken from: http://www.open3d.org/docs/release/docker.html \
    libegl1 libgl1 libgomp1 \
    # Requirements for pytorch \
    libopenblas-dev libopenmpi-dev

# Create a virtual environment for python.
RUN python3 -m venv /opt/venv

# Install pytorch and torchvision (if SKIP_PYTORCH_INSTALL==0)
COPY docker/maybe_install_cuda_pytorch.sh /
RUN /maybe_install_cuda_pytorch.sh

# Install python deps.
RUN . /opt/venv/bin/activate && \
    python3 -m pip install --upgrade \
    # Deployment tools \
    wheel requests setuptools \
    # Testing \
    pytest pytest-rerunfailures \
    # Profiling \
    nvtx

# Install cmake.
COPY docker/install_cmake.sh /
RUN /install_cmake.sh

# Make sure we're using text-based interactive terminal.
ENV DEBIAN_FRONTEND=teletype

# Setup env variables by adding them to the global bashrc.
RUN . /opt/venv/bin/activate && python3 -c "import site; print(site.getsitepackages()[0])" > /site-packages-dir
RUN touch /etc/nvblox_env.sh
RUN echo 'PATH=$PATH:/usr/local/cuda/bin' | tee --append /etc/nvblox_env.sh && \
    # Make sure we have the torch cmake path
    echo "export CMAKE_PREFIX_PATH=$(cat /site-packages-dir)/torch" | tee --append /etc/nvblox_env.sh && \
    # Explicit CUDA_PATH is needed here due to the non-standard location of the nvcc compiler (see section on ccache)
    echo 'export CUDA_PATH=/usr/local/cuda' | tee --append /etc/nvblox_env.sh && \
    # Always activate the venv upon login \
    echo 'source /opt/venv/bin/activate' | tee --append /etc/nvblox_env.sh
RUN echo 'source /etc/nvblox_env.sh' | tee --append /etc/bash.bashrc

# Ensure that a local user can access the python venv
RUN chmod -R a+rw /opt/venv

COPY src/nvblox_torch /rabbit/src/nvblox_torch
RUN . /opt/venv/bin/activate && python3 -m pip install /rabbit/src/nvblox_torch