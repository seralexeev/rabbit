ARG BASE_IMAGE=nvcr.io/nvidia/cuda:12.8.0-devel-ubuntu24.04
FROM ${BASE_IMAGE}

# TZData goes first.
RUN apt-get update
ENV TZ Europe/Berlin
ENV DEBIAN_FRONTEND noninteractive
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
RUN apt-get update && apt-get install -y tzdata

# Install apt dependencies.
RUN apt update && apt-get --no-install-recommends install -y \
    # Source development \
    git jq build-essential \
    # System packages \
    sudo ssh gnupg wget curl unzip \
    # Package management tools \
    apt-utils software-properties-common \
    # Python bootstrap packages \
    python3-pip python3-venv \
    # Open3D system dependencies \
    # NOTE(alexmillane): Taken from: http://www.open3d.org/docs/release/docker.html \
    libegl1 libgl1 libgomp1 \
    # Tools used in tests \
    cuda-nsight-systems-12-6 parallel

# Install pytorch.
# Note that all python deps are installed inside a venv. This is required by ubuntu >= 24.
RUN python3 -m venv /opt/venv
RUN . /opt/venv/bin/activate && \
    python3 -m pip install --ignore-installed --upgrade pip && \
    python3 -m pip install \
    # Pytorch. Note that this version is only compatible with pre-cxx11 ABI binaries. \
    torch==2.3

# Install python deps.
RUN . /opt/venv/bin/activate && \
    python3 -m pip install --upgrade \
    # Deployment tools \
    wheel requests setuptools \
    # Testing \
    pytest pytest-rerunfailures \
    # Profiling \
    nvtx

# Install cmake.
COPY docker/install_cmake.sh /
RUN /install_cmake.sh

# Install ccache. We need a recent version for improved nvcc support.
COPY docker/install_ccache.sh /
RUN /install_ccache.sh

# Make sure we're using text-based interactive terminal.
ENV DEBIAN_FRONTEND teletype

# Setup env variables by adding them to the global bashrc.
RUN . /opt/venv/bin/activate && python3 -c "import site; print(site.getsitepackages()[0])" > /site-packages-dir
RUN touch /etc/nvblox_env.sh
RUN echo 'PATH=$PATH:/usr/local/cuda/bin' | tee --append /etc/nvblox_env.sh && \
    # Make sure we have the torch cmake path
    echo "export CMAKE_PREFIX_PATH=$(cat /site-packages-dir)/torch" | tee --append /etc/nvblox_env.sh && \
    # Explicit CUDA_PATH is needed here due to the non-standard location of the nvcc compiler (see section on ccache)
    echo 'export CUDA_PATH=/usr/local/cuda' | tee --append /etc/nvblox_env.sh && \
    # Always activate the venv upon login \
    echo 'source /opt/venv/bin/activate' | tee --append /etc/nvblox_env.sh
RUN echo 'source /etc/nvblox_env.sh' | tee --append /etc/bash.bashrc

# Ensure that a local user can access the python venv
RUN chmod -R a+rw /opt/venv
